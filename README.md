# Formula-1-Regressor-Model

I built a random forest regressor model with the purpose of predicting a Formula 1 team's point total for a single season. I was curious to figure out what features correlate strongly with points in order to figure out a Formula 1 team's season point total.  

I chose to do the Formula 1 for my individual project because I'm an avid Formula 1 fan and my group for th egroup project chose a Formula 1 dataset as one of our potential three options. 
The Formula 1 World Championship (1950-2023) datset comes from kaggle. The dataset is  made up of fourteen contected datasets.(add a picture of the daatsets) 

I chose to only do five of those fourteen datasets because my computer doesn't have the computational capability to handle the entire dataset. The five daatsets I chose were the results, races, constructors, constructors_standings, and constructors results. (picture of the datasets and their respective columns)

I merged these datasets using pd.merge because these datasets were connected by key columns. I also needed to merge these datasets in order to have a singular dataset to train the model on. 
(show a pciture of each of the merging. ) (show a picture of the merged dataset)

The dataset is prone to outliers. For example, in the year 1988 Mclaren won the constructor's championship with a total 199 points which was 134 more points than Ferrari who finished in second that season. I chose not to get rid of outliers since they are true values. If I got rid of the outliers then the dataset would become inacurate due to the loss of true values. 

I did make a histogram of the target variable points to see if it was skewed which ended up being the case as the target variable was right skewed. This right skewed occured due to outliers but since I didn't want to get rid of outliers since they represent true values, I instead chose to perform a log transformation on the target variable in order to give it a more even distribution. (explain why you want an even distribution). (explain why you didn't do this to the predictor variables). (picture of the histogram)

I tried to scale the predictor variables through the use of a standard scaler to ensure all the predictor variables were all on the same scale to avoid introducing bias into the dataset by some variables scale giving them mor eimportance than other variables. To my suprise, the scaler didn't improve the mean squared error of the model at all. (try other forms of scaling and explain that)

I chose to get rid of all null values because they don't provide any data to the model and because my dataset had very few null values. (picture of null values)

I did have to convert all my categorical data columns to numerical ones because a regessor model requires that all of its features be numerical data. I accomplished this through the use of one hot encoding with pd.get_dummies because it converts each category in the column into a dummy variable. 

I chose to use a regressor random forest model because I needed to use a regressor model in order to predict the points for a team and I chose random forest because of its ability to avoid overfitting the data. I also chose the random forest regressor become it runs effecentially on large datasets such as Formula 1 World Championship(1950-2023). I used the random forest regressor model's most important features to not only figure out which features best contributed to the dataset but also how well they contributed to the dataset. I then used this information to get rid of features that had a very weak correlation because they provided little to no useful data to the model. (mention your model's initially high mean squared error) 

I also built a scatterplot of the model's predicted vs actual values to see what was causing my model's high mean squared error. After looking at this model I noticed that the model had trouble predicting outliers because they are so rare but I also noticed that there was a vast difference in the accumalation of points overtime. Upon further research, I discovered that this was due to the fact that Formula 1 has changed its points system over time. From the years 1962 to 1990 the point system was 9, 6, 4, 3, 2, 1 points but in the year 1991 to 2002 that point system was changed to 10, 8, 6, 5, 4, 3, 2, 1 points and was then changed again in the year 2003 to 25, 18, 15, 12, 10, 8, 6, 4, 2, 1. (show a picture of the scatterplot) The reason for my model's high mean sqaured error was because of the fact that I was training the model on multiple different point systems which made it diffcult for the model to accuratly predict the points for a constructor in a season because it didn't have a singular system to go off of. I decided to fix this error by only using data between the years 1961 and 1990 since the point system remained the same during that time period which gives the model a singular point system to use. I found that I can predict a constructor's points in a season based off my chosen features because the model only had a mean squared error of 0.003. I used a scatter plot to compare the model's predicted values to its actual values. This model shows that its features are important to the outcome of a Formula 1 team's season point total which can show Formula 1 teams what to focus on in order to get as many points as possible. My model results are similar to what other researchers ahve found like the fact that better lap times and startin gpositoin correlate with more points accumalted. This model can be used as a base model for future research as more features can be added on such as pit stop time, qualifying time, etc in order to find other features that have an impact on a Formula 1 team's season point total. This model can also be used to predict how a team will do in a season which can allow that team to see if can reach their goals for that season with the way things currently are or is there a need for improvement. 

